# AnyProxy v2 AI Services Configuration
# Expose local AI models and services securely

log:
  level: "info"
  format: "json"
  output: "stdout"

# Use gRPC for better AI performance
transport:
  type: "grpc"

# Gateway (public server)
gateway:
  listen_addr: ":9090"
  tls_cert: "certs/server.crt"
  tls_key: "certs/server.key"
  auth_username: "ai_admin"
  auth_password: "ai_secure_2024"

# HTTP proxy for AI REST APIs
proxy:
  http:
    listen_addr: ":8080"
    auth_username: "ai_user"
    auth_password: "ai_password"

# Client (AI server/workstation)
client:
  gateway_addr: "YOUR_GATEWAY_IP:9090"
  gateway_tls_cert: "certs/server.crt"
  client_id: "ai-server"
  group_id: "ai"
  replicas: 2
  auth_username: "ai_admin"
  auth_password: "ai_secure_2024"
  
  # Security for AI services
  forbidden_hosts:
    - "169.254.0.0/16"        # Cloud metadata
    - "127.0.0.1"
    - "localhost"
  
  allowed_hosts:
    - "localhost:8000"        # AI API server
    - "localhost:8001"        # Alternative port
    - "localhost:11434"       # Ollama
    - "localhost:7860"        # Gradio
    - "localhost:8501"        # Streamlit

# Usage examples:
# OpenAI-compatible API:
# curl -x http://ai_user:ai_password@YOUR_GATEWAY_IP:8080 \
#   -H "Content-Type: application/json" \
#   -d '{"model": "gpt-3.5-turbo", "messages": [{"role": "user", "content": "Hello!"}]}' \
#   http://localhost:8000/v1/chat/completions
#
# Ollama:
# curl -x http://ai_user:ai_password@YOUR_GATEWAY_IP:8080 \
#   -d '{"model": "llama2", "prompt": "Why is the sky blue?"}' \
#   http://localhost:11434/api/generate 